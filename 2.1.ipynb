{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moni/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# path to checkpoint\n",
    "checkpoint_folder = \"./New Folder With Items\"\n",
    "\n",
    "# loading model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_folder)\n",
    "\n",
    "# loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(checkpoint_folder, dataset_name, split='test', json=False):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_folder)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_folder)\n",
    "    \n",
    "    if json:\n",
    "        dataset = load_dataset('json', data_files=dataset_name, split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples['premise'], examples['hypothesis'], truncation=True)\n",
    "\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    results = trainer.predict(dataset)\n",
    "\n",
    "    preds, labels = get_predictions_and_labels(results)\n",
    "\n",
    "    overall_accuracy = calculate_overall_accuracy(preds, labels)\n",
    "    print(f\"Overall accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    accuracy_per_class = calculate_class_accuracy(preds, labels)\n",
    "    print(f\"Accuracy per class: {accuracy_per_class}\")\n",
    "\n",
    "    comparison_table = create_comparison_table(dataset, preds, labels)\n",
    "    return comparison_table, overall_accuracy, accuracy_per_class\n",
    "\n",
    "def get_predictions_and_labels(results):\n",
    "    preds = np.argmax(results.predictions, axis=1)\n",
    "    labels = results.label_ids\n",
    "    return preds, labels\n",
    "\n",
    "def calculate_overall_accuracy(predictions, labels):\n",
    "    return np.sum(predictions == labels) / len(labels)\n",
    "\n",
    "def calculate_class_accuracy(predictions, labels):\n",
    "    class_labels = [0, 1, 2] # 0: entailment, 1: neutral, 2: contradiction\n",
    "    accuracies = {}\n",
    "    for class_label in class_labels:\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "        correct_predictions = np.sum(predictions[class_indices] == labels[class_indices])\n",
    "        accuracies[class_label] = correct_predictions / len(class_indices)\n",
    "    return accuracies\n",
    "\n",
    "def create_comparison_table(dataset, predictions, labels):\n",
    "    wrong_indices = np.where(predictions != labels)[0]\n",
    "    comparison_data = {\n",
    "        \"Premise\": [dataset[int(i)][\"premise\"] for i in wrong_indices[:10]], \n",
    "        \"Hypothesis\": [dataset[int(i)][\"hypothesis\"] for i in wrong_indices[:10]], \n",
    "        \"Ground Truth\": [labels[int(i)] for i in wrong_indices[:10]],\n",
    "        \"Prediction\": [predictions[int(i)] for i in wrong_indices[:10]], \n",
    "    }\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n",
    "\n",
    "def analyze_attention(model_checkpoint, premise, hypothesis, device='cpu'):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, output_attentions=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # extract attention weights\n",
    "    attention_weights, inputs = get_attention_weights(model, tokenizer, premise, hypothesis, device=device)\n",
    "\n",
    "    # get average attention for each token across all heads and layers\n",
    "    avg_attention_matrix = average_attention_for_each_token(attention_weights)\n",
    "\n",
    "    # get input tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # plot average attention matrix\n",
    "    plot_avg_attention_matrix(avg_attention_matrix, tokens)\n",
    "\n",
    "# function to extract attention weights\n",
    "def get_attention_weights(model, tokenizer, premise, hypothesis, device='cpu'):\n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True).to(device)\n",
    "    model = model.to(device) \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # get attention weights from model outputs\n",
    "    attention_weights = outputs.attentions\n",
    "    return attention_weights, inputs\n",
    "\n",
    "# function to calculate average attention for each token across all heads and layers\n",
    "def average_attention_for_each_token(attention_weights):\n",
    "    num_layers = len(attention_weights)\n",
    "    num_heads = attention_weights[0].shape[1]\n",
    "    \n",
    "    # zero matrix for average attention\n",
    "    avg_attention = torch.zeros(attention_weights[0].shape[-1], attention_weights[0].shape[-1]).to(attention_weights[0].device)\n",
    "    \n",
    "    # sum attention across all heads and layers for each token pair\n",
    "    for layer_attention in attention_weights:\n",
    "        avg_attention += layer_attention.mean(dim=1)[0] \n",
    "    \n",
    "    # average across all layers\n",
    "    avg_attention /= num_layers # normalize by num layers\n",
    "    return avg_attention\n",
    "\n",
    "def plot_avg_attention_matrix(avg_attention_matrix, tokens):\n",
    "    # ensure it's on cpu for plotting\n",
    "    avg_attention_matrix = avg_attention_matrix.detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(avg_attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
    "    plt.title(\"Average attention across all layers and heads for each token\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85d3b054548475b9c0526206e3d82ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8733\n",
      "Accuracy per class: {0: 0.9103325415676959, 1: 0.8462255358807083, 2: 0.9091751621872104}\n"
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, 'snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of analyzing attention for a specific sentence pair\n",
    "# analyze_attention(checkpoint_folder, \"a man is riding a horse.\", \"a man is riding a bike.\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a37a7a43195467da5c1765218f72e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4871abbfe0d74be580aa9b28d22f0c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/777 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.5874\n",
      "Accuracy per class: {0: 0.8254716981132075, 1: 0.42429682824655895, 2: 0.6387349953831948}\n"
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_1.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of analyzing attention for a specific sentence pair\n",
    "# analyze_attention(checkpoint_folder, \"a man is riding a horse.\", \"a man is riding a bike.\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd233c90d13c42c7bde33368ba78ee2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318fa9b758df4e278a08e6a83264eb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7896\n",
      "Accuracy per class: {0: 0.786520190023753, 1: 0.7875116495806151, 2: 0.8378127896200185}\n"
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_2.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d7e2950751478196f504650b331f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d78d610df7a40fc82de813bb45e9a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb81a3990ee4616b25147d407e78cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8733\n",
      "Accuracy per class: {0: 0.9103325415676959, 1: 0.8462255358807083, 2: 0.9091751621872104}\n"
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_3.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262547f536ad42c9b42efae6b6797218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c573b6e32fb94c6da12d5004106dc8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b91e4e08d9475e85998e208748b52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8692\n",
      "Accuracy per class: {0: 0.9207221350078493, 1: 0.8096774193548387, 2: 0.8702912102591505}\n"
     ]
    }
   ],
   "source": [
    " # example of using the generalized functions for snli dataset\n",
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_4.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed4e6bf65724ca7a1d2d1d7516b0543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65efe9e11a3d4cb59a709fe9228677a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c952339f049a471989c23bb5aa685160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.5791\n",
      "Accuracy per class: {0: 0.09619952494061758, 1: 0.7651444547996272, 2: 0.9280197713932654}\n"
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_5.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f47f1c48f0b470fb603b7eef7ae300c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b59b3dca106461b8c2955b226a69111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of using the generalized functions for SNLI dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./set_6.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36mevaluate_model_on_dataset\u001b[0;34m(checkpoint_folder, dataset_name, split, json)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m], examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypothesis\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Evaluate the model using the Trainer API\u001b[39;00m\n\u001b[1;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/NLP/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/NLP/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/NLP/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3438\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3434\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3435\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3436\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3438\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NLP/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mevaluate_model_on_dataset.<locals>.preprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpremise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypothesis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3055\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/NLP/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3117\u001b[0m     )\n\u001b[1;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m-> 3120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3123\u001b[0m     )\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words:\n\u001b[1;32m   3126\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m text \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_6.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4d12a28f4945679c0e89f40bfbac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdd42c6e9dc41fbab65cac2fc2b6377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479826be38e74890949b10fce4a987ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8701\n",
      "Accuracy per class: {0: 0.9290060851926978, 1: 0.8206583427922814, 2: 0.8654644111075647}\n"
     ]
    }
   ],
   "source": [
    "comparison_table_snli, overall_accuracy_snli, accuracy_per_class_snli = evaluate_model_on_dataset(checkpoint_folder, './set_7.jsonl', split='train', json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('snli', split='test')\n",
    "\n",
    "set_1 = dataset.select(range(len(dataset)))\n",
    "set_2 = dataset.select(range(len(dataset)))\n",
    "set_3 = dataset.select(range(len(dataset)))\n",
    "set_4 = dataset.select(range(len(dataset)))\n",
    "set_5 = dataset.select(range(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moni/NLP/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87ee03fe1a4407d85a483e7890daa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get bert tokenizer and models\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# load models and move to gpu \n",
    "model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device) # mlm model for word replacement\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased').to(device) # bert model for sentence embeddings\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy() \n",
    "\n",
    "# function to replace a word in a sentence \n",
    "def replace_word_with_bert(sentence, word_to_replace):\n",
    "    masked_sentence = sentence.replace(word_to_replace, '[MASK]', 1)\n",
    "    inputs = tokenizer(masked_sentence, return_tensors='pt').to(device)\n",
    "    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_mlm(**inputs).logits\n",
    "\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "# get top replacement word (that is not the original word)\n",
    "    for token in top_5_tokens:\n",
    "        replacement_word = tokenizer.decode([token])\n",
    "        if replacement_word.lower() != word_to_replace.lower():\n",
    "            return replacement_word\n",
    "    return word_to_replace\n",
    "\n",
    "# function to replace a word and calculate similarity\n",
    "def replace_word_and_get_similarity(sentence, word_to_replace):\n",
    "    replaced_word = replace_word_with_bert(sentence, word_to_replace)\n",
    "    replaced_sentence = sentence.replace(word_to_replace, replaced_word, 1)\n",
    "\n",
    "   # get embeddings for original and replaced sentences\n",
    "    original_embedding = get_sentence_embedding(sentence)\n",
    "    replaced_embedding = get_sentence_embedding(replaced_sentence)\n",
    "\n",
    "   # calculate cosine similarity between original and replaced sentence embeddings\n",
    "    similarity = cosine_similarity(original_embedding, replaced_embedding)[0][0]\n",
    "    return similarity, replaced_word, replaced_sentence\n",
    "\n",
    "# find the word whose replacement least changes the meaning\n",
    "def find_least_changed_word(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "   # identify nouns, adjectives in the sentence\n",
    "    candidates = [word for word, pos in pos_tags if pos in ['NN', 'NNS', 'JJ']]\n",
    "\n",
    "    best_similarity = -1\n",
    "    best_word = None\n",
    "    best_replaced_word = None\n",
    "    best_replaced_sentence = None\n",
    "\n",
    "   # for each candidate word, replace it and calculate similarity\n",
    "    for word in candidates:\n",
    "        similarity, replaced_word, replaced_sentence = replace_word_and_get_similarity(sentence, word)\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_word = word\n",
    "            best_replaced_word = replaced_word\n",
    "            best_replaced_sentence = replaced_sentence\n",
    "\n",
    "    return best_word, best_replaced_word, best_replaced_sentence, best_similarity\n",
    "\n",
    "# apply the least-changed-word replacement to the hypothesis field in a dataset row\n",
    "def replace_in_dataset(row):\n",
    "    sentence = row['hypothesis']\n",
    "    _, _, new_sentence, _ = find_least_changed_word(sentence)\n",
    "\n",
    "   # replace the hypothesis with the new sentence\n",
    "    row['hypothesis'] = new_sentence\n",
    "    return row\n",
    "\n",
    "# apply the function to each row in the dataset (e.g., replacing in 'hypothesis' field)\n",
    "set_3 = set_3.map(replace_in_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d465199ebe8f46d2807f25ade4c2d284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1505966"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_3.to_json(\"set_3.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "The church has cracks in the walls.\n",
      "1\n",
      "This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "The church is filled with people.\n",
      "0\n",
      "This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "A choir singing at a football game.\n",
      "2\n",
      "A woman with a green headscarf, blue shirt and a very big grin.\n",
      "The woman is dead.\n",
      "1\n",
      "A woman with a green headscarf, blue shirt and a very big grin.\n",
      "The woman is very beautiful.\n",
      "0\n",
      "A woman with a green headscarf, blue shirt and a very big grin.\n",
      "The film has been shot.\n",
      "2\n",
      "An old man with a package poses in front of an advertisement.\n",
      "A woman poses in front of an ad.\n",
      "0\n",
      "An old man with a package poses in front of an advertisement.\n",
      "A woman poses in front of an ad for beer.\n",
      "1\n",
      "An old man with a package poses in front of an advertisement.\n",
      "A woman walks by an ad.\n",
      "2\n",
      "A statue at a museum that no seems to be looking at.\n",
      "The painting is offensive and people are mad that it is on display.\n",
      "1\n",
      "A statue at a museum that no seems to be looking at.\n",
      "There is a statue that not all people seem to be interested in.\n",
      "0\n",
      "A statue at a museum that no seems to be looking at.\n",
      "thousands of people are gathered around the statue.\n",
      "2\n",
      "A land rover is being driven across a river.\n",
      "A Land Rover is splashing water as it crosses a road.\n",
      "0\n",
      "A land rover is being driven across a river.\n",
      "A vehicle is crossing a bridge.\n",
      "0\n",
      "A land rover is being driven across a river.\n",
      "A sedan is stuck in the bottom of a river.\n",
      "2\n",
      "A man playing an electric guitar on stage.\n",
      "A woman playing banjo on the floor.\n",
      "2\n",
      "A man playing an electric guitar on stage.\n",
      "A guy playing guitar on stage.\n",
      "0\n",
      "A man playing an electric guitar on stage.\n",
      "A band is performing for cash.\n",
      "1\n",
      "A blond-haired doctor and her African american assistant looking threw new medical manuals.\n",
      "A man is looking at a book\n",
      "0\n",
      "A blond-haired doctor and her African american assistant looking threw new medical manuals.\n",
      "A child is eating pb and j\n",
      "2\n",
      "A blond-haired doctor and her African american assistant looking threw new medical manuals.\n",
      "A student is studying\n",
      "1\n",
      "One tan girl with a wool hat is running and leaning over an object, while another person in a wool hat is sitting on the ground.\n",
      "A man runs into a wall\n",
      "2\n",
      "One tan girl with a wool hat is running and leaning over an object, while another person in a wool hat is sitting on the ground.\n",
      "A young girl runs leans over an object\n",
      "0\n",
      "One tan girl with a wool hat is running and leaning over an object, while another person in a wool hat is sitting on the ground.\n",
      "A man watches his son leap\n",
      "1\n",
      "A young family enjoys feeling ocean waves lap at their feet.\n",
      "A young man and girl take their child to the beach for the first time.\n",
      "1\n",
      "A young family enjoys feeling ocean waves lap at their feet.\n",
      "A family is out at a party.\n",
      "2\n",
      "A young family enjoys feeling ocean waves lap at their feet.\n",
      "A family is at the scene.\n",
      "0\n",
      "A couple walk hand in hand down a street.\n",
      "A group is walking together.\n",
      "0\n",
      "A couple walk hand in hand down a street.\n",
      "The former is married.\n",
      "1\n",
      "A couple walk hand in hand down a street.\n",
      "A couple is sitting on a couch.\n",
      "2\n",
      "3 young man in hoods standing in the middle of a quiet street facing the camera.\n",
      "Three men sit by a busy street bareheaded.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for e in set_3:\n",
    "    print(e['premise'])\n",
    "    print(e['hypothesis'])\n",
    "    print(e['label'])\n",
    "    total+=1\n",
    "    if total > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
